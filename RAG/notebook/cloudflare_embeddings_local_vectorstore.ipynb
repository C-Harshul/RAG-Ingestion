{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG pipeline (Cloudflare Workers AI Embeddings + Local ChromaDB)\n",
        "\n",
        "This notebook ingests PDF documents, chunks them, generates embeddings using Cloudflare Workers AI, and stores vectors in local ChromaDB.\n",
        "\n",
        "- Embedding model: `@cf/baai/bge-base-en-v1.5` (768-dim) via Cloudflare Workers AI\n",
        "- Vector DB: ChromaDB (local)\n",
        "- Input: PDFs under `../data/pdf_files`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/harshul/Development/RAG-Model/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Imports and setup\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import requests\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Any\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Optional progress bar\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except Exception:\n",
        "    def tqdm(x, **kwargs):\n",
        "        return x\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Environment configuration for Cloudflare embeddings\n",
        "CLOUDFLARE_ACCOUNT_ID = os.getenv(\"CLOUDFLARE_ACCOUNT_ID\", \"<YOUR_ACCOUNT_ID>\")\n",
        "CLOUDFLARE_API_TOKEN = os.getenv(\"CLOUDFLARE_API_TOKEN\", \"<YOUR_API_TOKEN>\")\n",
        "\n",
        "# Embedding model per Cloudflare docs\n",
        "CF_EMBEDDINGS_MODEL = \"@cf/baai/bge-base-en-v1.5\"  # 768-dim\n",
        "\n",
        "if any(v.startswith(\"<YOUR_\") for v in [CLOUDFLARE_ACCOUNT_ID, CLOUDFLARE_API_TOKEN]):\n",
        "    print(\"WARNING: Set CLOUDFLARE_ACCOUNT_ID and CLOUDFLARE_API_TOKEN in your environment or edit this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 PDF files to process\n",
            "\n",
            "Processing: Bill's_Windsurf_Shop_Invoice.pdf\n",
            "  ✓ Loaded 1 pages\n",
            "\n",
            "Processing: Amy's_Bird_Sanctuary_Invoice.pdf\n",
            "  ✓ Loaded 1 pages\n",
            "\n",
            "Processing: et-cod.pdf\n",
            "  ✓ Loaded 334 pages\n",
            "\n",
            "Processing: Cool_Cars_Invoice.pdf\n",
            "  ✓ Loaded 1 pages\n",
            "\n",
            "Processing: Dukes_Basketball_Camp_Invoice.pdf\n",
            "  ✓ Loaded 1 pages\n",
            "\n",
            "Processing: Diego_Rodriguez_Invoice.pdf\n",
            "  ✓ Loaded 1 pages\n",
            "\n",
            "Total documents loaded: 339\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "339"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PDF discovery and loading\n",
        "\n",
        "def process_all_pdfs(pdf_directory: str) -> List[Any]:\n",
        "    \"\"\"Load all PDFs found under a directory (recursive), returning LangChain Documents.\"\"\"\n",
        "    all_documents = []\n",
        "    pdf_dir = Path(pdf_directory)\n",
        "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
        "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "        try:\n",
        "            loader = PyPDFLoader(str(pdf_file))\n",
        "            documents = loader.load()\n",
        "            for doc in documents:\n",
        "                doc.metadata['source_file'] = pdf_file.name\n",
        "                doc.metadata['file_type'] = 'pdf'\n",
        "            all_documents.extend(documents)\n",
        "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
        "    return all_documents\n",
        "\n",
        "all_pdf_documents = process_all_pdfs(\"../data/pdf_files\")\n",
        "len(all_pdf_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split 339 documents into 1087 chunks\n",
            "Example chunk preview:\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            " ...\n",
            "{'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'source': \"../data/pdf_files/Bill's_Windsurf_Shop_Invoice.pdf\", 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': \"Bill's_Windsurf_Shop_Invoice.pdf\", 'file_type': 'pdf'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1087"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Chunking\n",
        "\n",
        "def split_documents(documents: List[Any], chunk_size=1000, chunk_overlap=200):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
        "    if split_docs:\n",
        "        print(\"Example chunk preview:\")\n",
        "        print(split_docs[0].page_content[:200], \"...\")\n",
        "        print(split_docs[0].metadata)\n",
        "    return split_docs\n",
        "\n",
        "chunks = split_documents(all_pdf_documents)\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cloudflare Workers AI Embeddings (REST client)\n",
        "\n",
        "class CFWorkersAIEmbeddings:\n",
        "    \"\"\"\n",
        "    Minimal client for Workers AI embeddings endpoint.\n",
        "    POST https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model}\n",
        "    Body: {\"text\": \"<string>\"}\n",
        "    Returns: {\"result\": {\"data\": [[...]]}} or {\"data\": [...]}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, account_id: str, api_token: str, model: str):\n",
        "        self.base = f\"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model}\"\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_token}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "\n",
        "    def embed_one(self, text: str, retries: int = 3, backoff: float = 1.5) -> np.ndarray:\n",
        "        payload = {\"text\": text}\n",
        "        last_err = None\n",
        "        for attempt in range(1, retries + 1):\n",
        "            try:\n",
        "                r = requests.post(self.base, headers=self.headers, json=payload, timeout=60)\n",
        "                if r.status_code == 200:\n",
        "                    data = r.json()\n",
        "                    vec = None\n",
        "                    if isinstance(data, dict) and \"result\" in data:\n",
        "                        result = data[\"result\"]\n",
        "                        if \"data\" in result and result[\"data\"]:\n",
        "                            first = result[\"data\"][0]\n",
        "                            vec = first if isinstance(first, list) else result[\"data\"]\n",
        "                    elif \"data\" in data:\n",
        "                        first = data[\"data\"][0] if isinstance(data[\"data\"], list) and data[\"data\"] and isinstance(data[\"data\"][0], list) else data[\"data\"]\n",
        "                        vec = first\n",
        "                    if vec is None:\n",
        "                        raise ValueError(f\"Unexpected response structure: {data}\")\n",
        "                    return np.array(vec, dtype=np.float32)\n",
        "                else:\n",
        "                    last_err = RuntimeError(f\"HTTP {r.status_code}: {r.text[:300]}\")\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "            time.sleep(backoff ** (attempt - 1))\n",
        "        raise last_err\n",
        "\n",
        "    def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "        vectors = []\n",
        "        for t in tqdm(texts, desc=\"Embedding with Cloudflare Workers AI\"):\n",
        "            vectors.append(self.embed_one(t))\n",
        "        return np.vstack(vectors)\n",
        "\n",
        "cf_embedder = CFWorkersAIEmbeddings(\n",
        "    account_id=CLOUDFLARE_ACCOUNT_ID,\n",
        "    api_token=CLOUDFLARE_API_TOKEN,\n",
        "    model=CF_EMBEDDINGS_MODEL,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store initialized. Collection: cloudflare_embeddings\n",
            "Existing documents in collection: 5523\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.VectorStore at 0x306ce5fd0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Local ChromaDB Vector Store (adapted from pdf_loader.ipynb)\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
        "    \n",
        "    def __init__(self, collection_name: str = \"cloudflare_embeddings\", persist_directory: str = \"../data/vector_store_cf\"):\n",
        "        \"\"\"\n",
        "        Initialize the vector store\n",
        "        \n",
        "        Args:\n",
        "            collection_name: Name of the ChromaDB collection\n",
        "            persist_directory: Directory to persist the vector store\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        \n",
        "        try:\n",
        "            # Initialize ChromaDB client\n",
        "            self.client = chromadb.PersistentClient(path=persist_directory)\n",
        "            \n",
        "            # Create or get collection\n",
        "            self.collection = self.client.get_or_create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"PDF document embeddings using Cloudflare Workers AI\"}\n",
        "            )\n",
        "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
        "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing vector store: {e}\")\n",
        "            raise\n",
        "\n",
        "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
        "        \"\"\"\n",
        "        Add documents and their embeddings to the vector store\n",
        "        \n",
        "        Args:\n",
        "            documents: List of LangChain documents\n",
        "            embeddings: Corresponding embeddings for the documents\n",
        "        \"\"\"\n",
        "        if len(documents) != len(embeddings):\n",
        "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
        "        \n",
        "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
        "        \n",
        "        # Prepare data for ChromaDB\n",
        "        ids = []\n",
        "        metadatas = []\n",
        "        documents_text = []\n",
        "        embeddings_list = []\n",
        "        \n",
        "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
        "            # Generate unique ID\n",
        "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "            ids.append(doc_id)\n",
        "            \n",
        "            # Metadata\n",
        "            metadata = dict(doc.metadata)\n",
        "            metadata[\"doc_index\"] = i\n",
        "            metadata[\"content_length\"] = len(doc.page_content)\n",
        "            metadatas.append(metadata)\n",
        "            \n",
        "            # Document text\n",
        "            documents_text.append(doc.page_content)\n",
        "            \n",
        "            # Embedding\n",
        "            embeddings_list.append(embedding.tolist())\n",
        "        \n",
        "        # Add to collection\n",
        "        try:\n",
        "            self.collection.add(\n",
        "                ids=ids,\n",
        "                embeddings=embeddings_list,\n",
        "                metadatas=metadatas,\n",
        "                documents=documents_text\n",
        "            )\n",
        "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
        "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error adding documents to vector store: {e}\")\n",
        "            raise\n",
        "\n",
        "    def query_similar(self, query_text: str, top_k: int = 5):\n",
        "        \"\"\"\n",
        "        Query for similar documents using Cloudflare embeddings\n",
        "        \n",
        "        Args:\n",
        "            query_text: Text to search for\n",
        "            top_k: Number of results to return\n",
        "        \"\"\"\n",
        "        # Generate embedding for query using Cloudflare\n",
        "        query_embedding = cf_embedder.embed_one(query_text)\n",
        "        \n",
        "        # Query the collection\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding.tolist()],\n",
        "            n_results=top_k\n",
        "        )\n",
        "        \n",
        "        return results\n",
        "\n",
        "vectorStore = VectorStore()\n",
        "vectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChromaDB Compaction Error Fix - Alternative approach with batching\n",
        "\n",
        "def add_documents_safely(vector_store, documents, embeddings, batch_size=20):\n",
        "    \"\"\"\n",
        "    Safely add documents to ChromaDB with batching to avoid compaction errors\n",
        "    \"\"\"\n",
        "    import time\n",
        "    \n",
        "    print(f\"Adding {len(documents)} documents in batches of {batch_size} to avoid compaction errors...\")\n",
        "    \n",
        "    total_added = 0\n",
        "    \n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_docs = documents[i:i + batch_size]\n",
        "        batch_embeddings = embeddings[i:i + batch_size]\n",
        "        \n",
        "        # Prepare batch data\n",
        "        ids = []\n",
        "        metadatas = []\n",
        "        documents_text = []\n",
        "        embeddings_list = []\n",
        "        \n",
        "        for j, (doc, embedding) in enumerate(zip(batch_docs, batch_embeddings)):\n",
        "            doc_id = f\"safe_batch_{i//batch_size}_doc_{uuid.uuid4().hex[:8]}_{j}\"\n",
        "            ids.append(doc_id)\n",
        "            \n",
        "            metadata = dict(doc.metadata)\n",
        "            metadata[\"doc_index\"] = i + j\n",
        "            metadata[\"batch_num\"] = i // batch_size\n",
        "            metadata[\"content_length\"] = len(doc.page_content)\n",
        "            metadatas.append(metadata)\n",
        "            \n",
        "            documents_text.append(doc.page_content)\n",
        "            embeddings_list.append(embedding.tolist())\n",
        "        \n",
        "        # Add batch with retry logic\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                vector_store.collection.add(\n",
        "                    ids=ids,\n",
        "                    embeddings=embeddings_list,\n",
        "                    metadatas=metadatas,\n",
        "                    documents=documents_text\n",
        "                )\n",
        "                total_added += len(batch_docs)\n",
        "                print(f\"✓ Batch {i//batch_size + 1}: Added {len(batch_docs)} documents. Total: {total_added}/{len(documents)}\")\n",
        "                break\n",
        "                \n",
        "            except Exception as e:\n",
        "                if \"compaction\" in str(e).lower() or \"hnsw\" in str(e).lower():\n",
        "                    print(f\"⚠️  Compaction error in batch {i//batch_size + 1}, attempt {attempt + 1}. Retrying...\")\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                    \n",
        "                    if attempt == max_retries - 1:\n",
        "                        print(f\"❌ Failed to add batch {i//batch_size + 1} after {max_retries} attempts\")\n",
        "                        print(f\"Error: {e}\")\n",
        "                        return total_added\n",
        "                else:\n",
        "                    print(f\"❌ Unexpected error in batch {i//batch_size + 1}: {e}\")\n",
        "                    return total_added\n",
        "        \n",
        "        # Small delay between batches to reduce load\n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    print(f\"\\n✅ Successfully added {total_added} documents to vector store!\")\n",
        "    print(f\"Total documents in collection: {vector_store.collection.count()}\")\n",
        "    return total_added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for 1087 chunks using Cloudflare Workers AI model: @cf/baai/bge-base-en-v1.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding with Cloudflare Workers AI: 100%|██████████| 1087/1087 [04:09<00:00,  4.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (1087, 768)\n",
            "Adding 1087 documents in batches of 15 to avoid compaction errors...\n",
            "✓ Batch 1: Added 15 documents. Total: 15/1087\n",
            "✓ Batch 2: Added 15 documents. Total: 30/1087\n",
            "✓ Batch 3: Added 15 documents. Total: 45/1087\n",
            "✓ Batch 4: Added 15 documents. Total: 60/1087\n",
            "✓ Batch 5: Added 15 documents. Total: 75/1087\n",
            "✓ Batch 6: Added 15 documents. Total: 90/1087\n",
            "✓ Batch 7: Added 15 documents. Total: 105/1087\n",
            "✓ Batch 8: Added 15 documents. Total: 120/1087\n",
            "✓ Batch 9: Added 15 documents. Total: 135/1087\n",
            "✓ Batch 10: Added 15 documents. Total: 150/1087\n",
            "✓ Batch 11: Added 15 documents. Total: 165/1087\n",
            "✓ Batch 12: Added 15 documents. Total: 180/1087\n",
            "✓ Batch 13: Added 15 documents. Total: 195/1087\n",
            "✓ Batch 14: Added 15 documents. Total: 210/1087\n",
            "✓ Batch 15: Added 15 documents. Total: 225/1087\n",
            "✓ Batch 16: Added 15 documents. Total: 240/1087\n",
            "✓ Batch 17: Added 15 documents. Total: 255/1087\n",
            "✓ Batch 18: Added 15 documents. Total: 270/1087\n",
            "✓ Batch 19: Added 15 documents. Total: 285/1087\n",
            "✓ Batch 20: Added 15 documents. Total: 300/1087\n",
            "✓ Batch 21: Added 15 documents. Total: 315/1087\n",
            "✓ Batch 22: Added 15 documents. Total: 330/1087\n",
            "✓ Batch 23: Added 15 documents. Total: 345/1087\n",
            "✓ Batch 24: Added 15 documents. Total: 360/1087\n",
            "✓ Batch 25: Added 15 documents. Total: 375/1087\n",
            "✓ Batch 26: Added 15 documents. Total: 390/1087\n",
            "✓ Batch 27: Added 15 documents. Total: 405/1087\n",
            "✓ Batch 28: Added 15 documents. Total: 420/1087\n",
            "✓ Batch 29: Added 15 documents. Total: 435/1087\n",
            "✓ Batch 30: Added 15 documents. Total: 450/1087\n",
            "✓ Batch 31: Added 15 documents. Total: 465/1087\n",
            "✓ Batch 32: Added 15 documents. Total: 480/1087\n",
            "✓ Batch 33: Added 15 documents. Total: 495/1087\n",
            "✓ Batch 34: Added 15 documents. Total: 510/1087\n",
            "✓ Batch 35: Added 15 documents. Total: 525/1087\n",
            "✓ Batch 36: Added 15 documents. Total: 540/1087\n",
            "✓ Batch 37: Added 15 documents. Total: 555/1087\n",
            "✓ Batch 38: Added 15 documents. Total: 570/1087\n",
            "✓ Batch 39: Added 15 documents. Total: 585/1087\n",
            "✓ Batch 40: Added 15 documents. Total: 600/1087\n",
            "✓ Batch 41: Added 15 documents. Total: 615/1087\n",
            "✓ Batch 42: Added 15 documents. Total: 630/1087\n",
            "✓ Batch 43: Added 15 documents. Total: 645/1087\n",
            "✓ Batch 44: Added 15 documents. Total: 660/1087\n",
            "✓ Batch 45: Added 15 documents. Total: 675/1087\n",
            "✓ Batch 46: Added 15 documents. Total: 690/1087\n",
            "✓ Batch 47: Added 15 documents. Total: 705/1087\n",
            "✓ Batch 48: Added 15 documents. Total: 720/1087\n",
            "✓ Batch 49: Added 15 documents. Total: 735/1087\n",
            "✓ Batch 50: Added 15 documents. Total: 750/1087\n",
            "✓ Batch 51: Added 15 documents. Total: 765/1087\n",
            "✓ Batch 52: Added 15 documents. Total: 780/1087\n",
            "✓ Batch 53: Added 15 documents. Total: 795/1087\n",
            "✓ Batch 54: Added 15 documents. Total: 810/1087\n",
            "✓ Batch 55: Added 15 documents. Total: 825/1087\n",
            "✓ Batch 56: Added 15 documents. Total: 840/1087\n",
            "✓ Batch 57: Added 15 documents. Total: 855/1087\n",
            "✓ Batch 58: Added 15 documents. Total: 870/1087\n",
            "✓ Batch 59: Added 15 documents. Total: 885/1087\n",
            "✓ Batch 60: Added 15 documents. Total: 900/1087\n",
            "✓ Batch 61: Added 15 documents. Total: 915/1087\n",
            "✓ Batch 62: Added 15 documents. Total: 930/1087\n",
            "✓ Batch 63: Added 15 documents. Total: 945/1087\n",
            "✓ Batch 64: Added 15 documents. Total: 960/1087\n",
            "✓ Batch 65: Added 15 documents. Total: 975/1087\n",
            "✓ Batch 66: Added 15 documents. Total: 990/1087\n",
            "✓ Batch 67: Added 15 documents. Total: 1005/1087\n",
            "✓ Batch 68: Added 15 documents. Total: 1020/1087\n",
            "✓ Batch 69: Added 15 documents. Total: 1035/1087\n",
            "✓ Batch 70: Added 15 documents. Total: 1050/1087\n",
            "✓ Batch 71: Added 15 documents. Total: 1065/1087\n",
            "✓ Batch 72: Added 15 documents. Total: 1080/1087\n",
            "✓ Batch 73: Added 7 documents. Total: 1087/1087\n",
            "\n",
            "✅ Successfully added 1087 documents to vector store!\n",
            "Total documents in collection: 6610\n",
            "\n",
            "✅ Successfully stored 1087 documents with Cloudflare embeddings in local ChromaDB!\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings using Cloudflare Workers AI and store safely\n",
        "\n",
        "texts = [c.page_content for c in chunks]\n",
        "print(f\"Generating embeddings for {len(texts)} chunks using Cloudflare Workers AI model: {CF_EMBEDDINGS_MODEL}\")\n",
        "embeddings = cf_embedder.embed_batch(texts)\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "# Store in the local ChromaDB vector database using safe batching\n",
        "added_count = add_documents_safely(vectorStore, chunks, embeddings, batch_size=15)\n",
        "print(f\"\\n✅ Successfully stored {added_count} documents with Cloudflare embeddings in local ChromaDB!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query results using Cloudflare embeddings:\n",
            "\n",
            "Result 1 (distance: 0.2168):\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "...\n",
            "\n",
            "Result 2 (distance: 0.2168):\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "...\n",
            "\n",
            "Result 3 (distance: 0.2168):\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# Test similarity query to validate ingestion\n",
        "\n",
        "def query_similar(text: str, top_k: int = 3):\n",
        "    results = vectorStore.query_similar(text, top_k=top_k)\n",
        "    return results\n",
        "\n",
        "# Test query\n",
        "result = query_similar(\"What is in the invoice for Bill's Windsurf Shop?\", top_k=3)\n",
        "print(\"Query results using Cloudflare embeddings:\")\n",
        "for i, (doc, distance) in enumerate(zip(result['documents'][0], result['distances'][0])):\n",
        "    print(f\"\\nResult {i+1} (distance: {distance:.4f}):\")\n",
        "    print(doc[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RAG Retriever initialized with Cloudflare embeddings and ChromaDB!\n"
          ]
        }
      ],
      "source": [
        "# RAG Retriever - Integrated with Cloudflare Embeddings and ChromaDB\n",
        "\n",
        "class RAGRetriever:\n",
        "    \"\"\"\n",
        "    RAG Retriever that integrates Cloudflare Workers AI embeddings with ChromaDB vector store\n",
        "    for semantic document retrieval and context generation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vector_store: VectorStore, embedder: CFWorkersAIEmbeddings, \n",
        "                 similarity_threshold: float = 0.7, max_context_length: int = 4000):\n",
        "        \"\"\"\n",
        "        Initialize RAG Retriever\n",
        "        \n",
        "        Args:\n",
        "            vector_store: ChromaDB VectorStore instance\n",
        "            embedder: Cloudflare Workers AI embeddings client\n",
        "            similarity_threshold: Minimum similarity score for relevant results\n",
        "            max_context_length: Maximum characters in combined context\n",
        "        \"\"\"\n",
        "        self.vector_store = vector_store\n",
        "        self.embedder = embedder\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.max_context_length = max_context_length\n",
        "    \n",
        "    def retrieve(self, query: str, top_k: int = 5, include_metadata: bool = True) -> dict:\n",
        "        \"\"\"\n",
        "        Retrieve relevant documents for a given query\n",
        "        \n",
        "        Args:\n",
        "            query: Search query text\n",
        "            top_k: Number of top results to retrieve\n",
        "            include_metadata: Whether to include document metadata\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with retrieved documents, scores, and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get similar documents from vector store\n",
        "            results = self.vector_store.query_similar(query, top_k=top_k)\n",
        "            \n",
        "            # Process and filter results\n",
        "            processed_results = self._process_results(results, include_metadata)\n",
        "            \n",
        "            return processed_results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {e}\")\n",
        "            return {\"documents\": [], \"scores\": [], \"metadata\": [], \"context\": \"\"}\n",
        "    \n",
        "    def _process_results(self, raw_results: dict, include_metadata: bool = True) -> dict:\n",
        "        \"\"\"\n",
        "        Process raw ChromaDB results into structured format\n",
        "        \n",
        "        Args:\n",
        "            raw_results: Raw results from ChromaDB query\n",
        "            include_metadata: Whether to include metadata\n",
        "            \n",
        "        Returns:\n",
        "            Processed results dictionary\n",
        "        \"\"\"\n",
        "        documents = raw_results.get('documents', [[]])[0]\n",
        "        distances = raw_results.get('distances', [[]])[0]\n",
        "        metadatas = raw_results.get('metadatas', [[]])[0] if include_metadata else []\n",
        "        ids = raw_results.get('ids', [[]])[0]\n",
        "        \n",
        "        # Convert distances to similarity scores (1 - distance for cosine similarity)\n",
        "        similarity_scores = [1 - dist for dist in distances]\n",
        "        \n",
        "        # Filter by similarity threshold\n",
        "        filtered_results = []\n",
        "        for i, score in enumerate(similarity_scores):\n",
        "            if score >= self.similarity_threshold:\n",
        "                result_item = {\n",
        "                    'document': documents[i],\n",
        "                    'score': score,\n",
        "                    'distance': distances[i],\n",
        "                    'id': ids[i]\n",
        "                }\n",
        "                if include_metadata and i < len(metadatas):\n",
        "                    result_item['metadata'] = metadatas[i]\n",
        "                filtered_results.append(result_item)\n",
        "        \n",
        "        # Sort by similarity score (highest first)\n",
        "        filtered_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        \n",
        "        # Generate combined context\n",
        "        context = self._generate_context([item['document'] for item in filtered_results])\n",
        "        \n",
        "        return {\n",
        "            'results': filtered_results,\n",
        "            'documents': [item['document'] for item in filtered_results],\n",
        "            'scores': [item['score'] for item in filtered_results],\n",
        "            'metadata': [item.get('metadata', {}) for item in filtered_results],\n",
        "            'context': context,\n",
        "            'num_results': len(filtered_results)\n",
        "        }\n",
        "    \n",
        "    def _generate_context(self, documents: list) -> str:\n",
        "        \"\"\"\n",
        "        Generate combined context from retrieved documents\n",
        "        \n",
        "        Args:\n",
        "            documents: List of document texts\n",
        "            \n",
        "        Returns:\n",
        "            Combined context string\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            return \"\"\n",
        "        \n",
        "        context_parts = []\n",
        "        current_length = 0\n",
        "        \n",
        "        for i, doc in enumerate(documents):\n",
        "            # Add document with separator\n",
        "            doc_text = f\"[Document {i+1}]\\n{doc}\\n\"\n",
        "            \n",
        "            # Check if adding this document exceeds max length\n",
        "            if current_length + len(doc_text) > self.max_context_length:\n",
        "                # Truncate the document to fit\n",
        "                remaining_space = self.max_context_length - current_length - 20  # Leave space for truncation indicator\n",
        "                if remaining_space > 100:  # Only add if there's meaningful space\n",
        "                    truncated_doc = f\"[Document {i+1}]\\n{doc[:remaining_space]}...\\n\"\n",
        "                    context_parts.append(truncated_doc)\n",
        "                break\n",
        "            \n",
        "            context_parts.append(doc_text)\n",
        "            current_length += len(doc_text)\n",
        "        \n",
        "        return \"\\n\".join(context_parts)\n",
        "    \n",
        "    def retrieve_with_reranking(self, query: str, top_k: int = 10, final_k: int = 5) -> dict:\n",
        "        \"\"\"\n",
        "        Retrieve documents with simple reranking based on query term overlap\n",
        "        \n",
        "        Args:\n",
        "            query: Search query text\n",
        "            top_k: Initial number of results to retrieve\n",
        "            final_k: Final number of results after reranking\n",
        "            \n",
        "        Returns:\n",
        "            Reranked results dictionary\n",
        "        \"\"\"\n",
        "        # Get initial results\n",
        "        initial_results = self.retrieve(query, top_k=top_k)\n",
        "        \n",
        "        if not initial_results['results']:\n",
        "            return initial_results\n",
        "        \n",
        "        # Simple reranking based on query term overlap\n",
        "        query_terms = set(query.lower().split())\n",
        "        \n",
        "        for result in initial_results['results']:\n",
        "            doc_terms = set(result['document'].lower().split())\n",
        "            term_overlap = len(query_terms.intersection(doc_terms)) / len(query_terms)\n",
        "            \n",
        "            # Combine semantic similarity with term overlap\n",
        "            result['rerank_score'] = 0.7 * result['score'] + 0.3 * term_overlap\n",
        "        \n",
        "        # Sort by rerank score and take top final_k\n",
        "        reranked = sorted(initial_results['results'], key=lambda x: x['rerank_score'], reverse=True)[:final_k]\n",
        "        \n",
        "        # Update the results\n",
        "        context = self._generate_context([item['document'] for item in reranked])\n",
        "        \n",
        "        return {\n",
        "            'results': reranked,\n",
        "            'documents': [item['document'] for item in reranked],\n",
        "            'scores': [item['rerank_score'] for item in reranked],\n",
        "            'metadata': [item.get('metadata', {}) for item in reranked],\n",
        "            'context': context,\n",
        "            'num_results': len(reranked)\n",
        "        }\n",
        "    \n",
        "    def get_source_info(self, results: dict) -> list:\n",
        "        \"\"\"\n",
        "        Extract source information from retrieval results\n",
        "        \n",
        "        Args:\n",
        "            results: Results dictionary from retrieve method\n",
        "            \n",
        "        Returns:\n",
        "            List of source information dictionaries\n",
        "        \"\"\"\n",
        "        sources = []\n",
        "        for metadata in results.get('metadata', []):\n",
        "            if metadata:\n",
        "                source_info = {\n",
        "                    'file': metadata.get('source_file', 'Unknown'),\n",
        "                    'page': metadata.get('page', 'Unknown'),\n",
        "                    'content_length': metadata.get('content_length', 0)\n",
        "                }\n",
        "                sources.append(source_info)\n",
        "        return sources\n",
        "\n",
        "# Initialize RAG Retriever with existing components\n",
        "rag_retriever = RAGRetriever(\n",
        "    vector_store=vectorStore,\n",
        "    embedder=cf_embedder,\n",
        "    similarity_threshold=0.6,  # Adjust based on your needs\n",
        "    max_context_length=3000\n",
        ")\n",
        "\n",
        "print(\"✅ RAG Retriever initialized with Cloudflare embeddings and ChromaDB!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Test Query 1: What is in the invoice for Bill's Windsurf Shop?\n",
            "============================================================\n",
            "\n",
            "Found 3 relevant documents:\n",
            "\n",
            "--- Result 1 (Similarity: 0.783) ---\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "...\n",
            "Source: Bill's_Windsurf_Shop_Invoice.pdf, Page: 0\n",
            "\n",
            "--- Result 2 (Similarity: 0.783) ---\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "...\n",
            "Source: Bill's_Windsurf_Shop_Invoice.pdf, Page: 0\n",
            "\n",
            "--- Result 3 (Similarity: 0.783) ---\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "...\n",
            "Source: Bill's_Windsurf_Shop_Invoice.pdf, Page: 0\n",
            "\n",
            "--- Combined Context (first 300 chars) ---\n",
            "[Document 1]\n",
            "Invoice for Bill's Windsurf Shop\n",
            "Email: Surf@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "Total $1200.00\n",
            "Invoice Date: October 20, 2025\n",
            "Due Date: November 19, 2025\n",
            "\n",
            "[Document 2]...\n",
            "\n",
            "============================================================\n",
            "Test Query 2: Tell me about Amy's Bird Sanctuary invoice details\n",
            "============================================================\n",
            "\n",
            "Found 3 relevant documents:\n",
            "\n",
            "--- Result 1 (Similarity: 0.766) ---\n",
            "Invoice for Amy's Bird Sanctuary\n",
            "Email: Birds@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00...\n",
            "Source: Amy's_Bird_Sanctuary_Invoice.pdf, Page: 0\n",
            "\n",
            "--- Result 2 (Similarity: 0.766) ---\n",
            "Invoice for Amy's Bird Sanctuary\n",
            "Email: Birds@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00...\n",
            "Source: Amy's_Bird_Sanctuary_Invoice.pdf, Page: 0\n",
            "\n",
            "--- Result 3 (Similarity: 0.766) ---\n",
            "Invoice for Amy's Bird Sanctuary\n",
            "Email: Birds@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00...\n",
            "Source: Amy's_Bird_Sanctuary_Invoice.pdf, Page: 0\n",
            "\n",
            "--- Combined Context (first 300 chars) ---\n",
            "[Document 1]\n",
            "Invoice for Amy's Bird Sanctuary\n",
            "Email: Birds@Intuit.com\n",
            "Invoice Details:\n",
            "Description Qty Unit Price Amount\n",
            "Design Service 1 $500.00 $500.00\n",
            "Consulting 2 $200.00 $400.00\n",
            "Installation 1 $300.00 $300.00\n",
            "Total $1200.00\n",
            "Invoice Date: October 20, 2025\n",
            "Due Date: November 19, 2025\n",
            "\n",
            "[Document 2...\n",
            "\n",
            "============================================================\n",
            "Test Query 3: What are the building automation system requirements?\n",
            "============================================================\n",
            "\n",
            "Found 0 relevant documents:\n",
            "\n",
            "============================================================\n",
            "Test Query 4: Schneider Electric EcoStruxure features\n",
            "============================================================\n",
            "\n",
            "Found 0 relevant documents:\n"
          ]
        }
      ],
      "source": [
        "# Test the RAG Retriever\n",
        "\n",
        "def test_rag_retriever():\n",
        "    \"\"\"Test the RAG Retriever with sample queries\"\"\"\n",
        "    \n",
        "    test_queries = [\n",
        "        \"What is in the invoice for Bill's Windsurf Shop?\",\n",
        "        \"Tell me about Amy's Bird Sanctuary invoice details\",\n",
        "        \"What are the building automation system requirements?\",\n",
        "        \"Schneider Electric EcoStruxure features\"\n",
        "    ]\n",
        "    \n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Test Query {i}: {query}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Basic retrieval\n",
        "        results = rag_retriever.retrieve(query, top_k=3)\n",
        "        \n",
        "        print(f\"\\nFound {results['num_results']} relevant documents:\")\n",
        "        \n",
        "        for j, (doc, score) in enumerate(zip(results['documents'], results['scores'])):\n",
        "            print(f\"\\n--- Result {j+1} (Similarity: {score:.3f}) ---\")\n",
        "            print(doc[:200] + \"...\" if len(doc) > 200 else doc)\n",
        "            \n",
        "            # Show source info if available\n",
        "            if j < len(results['metadata']) and results['metadata'][j]:\n",
        "                metadata = results['metadata'][j]\n",
        "                source_file = metadata.get('source_file', 'Unknown')\n",
        "                page = metadata.get('page', 'Unknown')\n",
        "                print(f\"Source: {source_file}, Page: {page}\")\n",
        "        \n",
        "        # Show combined context (truncated)\n",
        "        if results['context']:\n",
        "            print(f\"\\n--- Combined Context (first 300 chars) ---\")\n",
        "            print(results['context'][:300] + \"...\" if len(results['context']) > 300 else results['context'])\n",
        "\n",
        "# Run the test\n",
        "test_rag_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are the costs and pricing details in the invoices?\n",
            "\n",
            "==================================================\n",
            "\n",
            "STANDARD RETRIEVAL:\n",
            "\n",
            "\n",
            "RERANKED RETRIEVAL:\n",
            "\n",
            "\n",
            "SOURCE INFORMATION:\n"
          ]
        }
      ],
      "source": [
        "# Advanced RAG Retrieval with Reranking\n",
        "\n",
        "def test_reranking():\n",
        "    \"\"\"Test the reranking functionality\"\"\"\n",
        "    \n",
        "    query = \"What are the costs and pricing details in the invoices?\"\n",
        "    \n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # Standard retrieval\n",
        "    standard_results = rag_retriever.retrieve(query, top_k=5)\n",
        "    print(\"\\nSTANDARD RETRIEVAL:\")\n",
        "    for i, (doc, score) in enumerate(zip(standard_results['documents'][:3], standard_results['scores'][:3])):\n",
        "        print(f\"\\n{i+1}. Score: {score:.3f}\")\n",
        "        print(doc[:150] + \"...\")\n",
        "    \n",
        "    # Reranked retrieval\n",
        "    reranked_results = rag_retriever.retrieve_with_reranking(query, top_k=8, final_k=3)\n",
        "    print(\"\\n\\nRERANKED RETRIEVAL:\")\n",
        "    for i, result in enumerate(reranked_results['results']):\n",
        "        print(f\"\\n{i+1}. Rerank Score: {result['rerank_score']:.3f} (Semantic: {result['score']:.3f})\")\n",
        "        print(result['document'][:150] + \"...\")\n",
        "    \n",
        "    # Source information\n",
        "    sources = rag_retriever.get_source_info(reranked_results)\n",
        "    print(\"\\n\\nSOURCE INFORMATION:\")\n",
        "    for i, source in enumerate(sources):\n",
        "        print(f\"{i+1}. File: {source['file']}, Page: {source['page']}\")\n",
        "\n",
        "# Test reranking\n",
        "test_reranking()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
